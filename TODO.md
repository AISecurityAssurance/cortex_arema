## ğŸš€ Planned Features

### ğŸ”Œ Backend Integration
- [ ] Connect to FastAPI (`/generate`) to fetch AWS Bedrock results
- [ ] Encode image to base64 and send with prompt
- [ ] Dynamically populate responses from backend

### ğŸ§  Prompt Templates & Modal UI
- [ ] Add â€œPrompt Templatesâ€ button with modal for structured prompt construction
  - [ ] STRIDE (Security threats)
  - [ ] MAESTRO (Mission-centric)
  - [ ] STPA-SEC (Systems-theoretic)
- [ ] Generate templated prompt into prompt input field

### ğŸ§ª Evaluation & Testing Features
- [ ] Built-in prompt injection test cases (for model safety testing)
- [ ] Response time measurement (latency benchmark)
- [ ] Detailed feedback options (fluency, hallucination, relevance, safety)
- [ ] Token-level risk flags (e.g., for insecure suggestions or info leaks)

### ğŸ‘¥ Collaboration & Sharing
- [ ] Share prompt + responses via unique URL
- [ ] Multi-user voting + consensus display

---

## ğŸ“Š Leaderboards & Analytics

- [ ] Add leaderboard UI for model win tracking
  - [ ] Show win/loss/tie counts per model
  - [ ] Track average response time
- [ ] Export results (PDF/Markdown)
- [ ] Session-level stats: which model wins most often, per prompt type

---

## ğŸ§© Model Management

- [ ] Add support for more models via dropdown
  - AWS Bedrock Titan, Mistral, Llama 3, Gemini
- [ ] Refactor model config into `[{ label, model_id }]`
- [ ] Prevent duplicate model selection (A â‰  B)

---

## ğŸ“ Usability Enhancements

- [ ] Prompt history panel with reuse support
- [ ] Inline commenting on responses
- [ ] Chat-like multi-turn mode (experimental)

---

## ğŸ›  Developer Ideas (Stretch)

- [ ] Webhooks to store prompt logs in Supabase
- [ ] Toggle Claude vs Gemini vs OpenAI formatting modes
- [ ] Token-level diff visualization (A vs B)
